{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Job Skill Analysis Notebook\n",
                "\n",
                "This notebook allows you to scrape job descriptions for specific roles and analyze the most common skills and keywords. \n",
                "It uses `scraper.py` from the JobSniper project.\n",
                "\n",
                "**Prerequisites**:\n",
                "- Ensure `clean_and_tokenize` logic is customized if you want to filter different stopwords."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install visualization library if missing\n",
                "%pip install matplotlib seaborn wordcloud"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scraper import scrape_job_boards\n",
                "import pandas as pd\n",
                "import re\n",
                "from collections import Counter\n",
                "import time\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_and_tokenize(text):\n",
                "    if not isinstance(text, str):\n",
                "        return []\n",
                "    \n",
                "    # Lowercase\n",
                "    text = text.lower()\n",
                "    \n",
                "    # Replace special separators with space\n",
                "    text = text.replace('/', ' ').replace(',', ' ').replace('(', ' ').replace(')', ' ')\n",
                "    \n",
                "    # Simple tokenization\n",
                "    tokens = text.split()\n",
                "    \n",
                "    # Stopwords list\n",
                "    stopwords = set([\n",
                "        'and', 'or', 'the', 'a', 'an', 'in', 'to', 'of', 'for', 'with', 'on', 'at', 'by', 'from', 'as',\n",
                "        'is', 'are', 'was', 'were', 'be', 'been', 'has', 'have', 'had', 'that', 'this', 'it', 'not',\n",
                "        'we', 'you', 'your', 'our', 'will', 'can', 'may', 'should', 'would', 'if', 'but', 'so',\n",
                "        'experience', 'years', 'work', 'job', 'role', 'team', 'skills', 'requirements', 'qualifications',\n",
                "        'description', 'looking', 'hiring', 'opportunity', 'company', 'business', 'development', 'software',\n",
                "        'engineer', 'engineering', 'developer', 'systems', 'solutions', 'services', 'technical', 'technology',\n",
                "        'strong', 'knowledge', 'understanding', 'proficiency', 'ability', 'preferred', 'plus', 'degree',\n",
                "        'bachelor', 'master', 'computer', 'science', 'related', 'field', 'working', 'environment', 'support',\n",
                "        'design', 'create', 'build', 'maintain', 'using', 'based', 'across', 'within', 'other', 'new',\n",
                "        'best', 'practices', 'full', 'time', 'part', 'contract', 'remote', 'location', 'us', 'application',\n",
                "        'project', 'projects', 'ensure', 'help', 'need', 'seeking', 'join', 'candidates', 'must', 'responsibilities',\n",
                "        'learning', 'data', 'analysis', 'analytics', 'management', 'product', 'production', 'code', 'coding',\n",
                "        'platform', 'infrastructure', 'tools', 'technologies', 'needs', 'highly', 'various', 'excellent',\n",
                "        'communication', 'written', 'verbal', 'collaborate', 'stakeholders', 'solve', 'complex', 'problems',\n",
                "        'performance', 'quality', 'standards', 'process', 'processes', 'improve', 'improvement', 'grow',\n",
                "        'growth', 'innovation', 'innovative', 'solutions', 'deliver', 'delivery', 'client', 'clients'\n",
                "    ])\n",
                "    \n",
                "    # Clean tokens\n",
                "    clean_tokens = []\n",
                "    for t in tokens:\n",
                "        # constant cleanup\n",
                "        t = t.strip('.,;:!?()[]{}\"\\'')\n",
                "        if len(t) > 1 and t not in stopwords:\n",
                "            clean_tokens.append(t)\n",
                "            \n",
                "    return clean_tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "roles = [\n",
                "    \"Machine Learning Engineer\",\n",
                "    \"Generative AI Engineer\", \n",
                "    \"Data Scientist\", \n",
                "    \"MLOps Engineer\", \n",
                "    \"Software Engineer\"\n",
                "]\n",
                "\n",
                "location = \"Remote\"\n",
                "num_jobs = 25\n",
                "hours_old = 72"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"--- Starting Skill Analysis ---\")\n",
                "print(f\"Roles: {roles}\")\n",
                "print(f\"Jobs per role: {num_jobs}\")\n",
                "print(\"-------------------------------\")\n",
                "\n",
                "all_descriptions = []\n",
                "\n",
                "for role in roles:\n",
                "    print(f\"Scraping {num_jobs} jobs for '{role}'...\")\n",
                "    try:\n",
                "        # enable linkedin_fetch_description for better data\n",
                "        df = scrape_job_boards(\n",
                "            role=role, \n",
                "            location=location, \n",
                "            num_jobs=num_jobs, \n",
                "            hours_old=hours_old,\n",
                "            linkedin_fetch_description=True\n",
                "        )\n",
                "        \n",
                "        if not df.empty and 'description' in df.columns:\n",
                "            descs = df['description'].dropna().tolist()\n",
                "            print(f\"  -> Found {len(df)} jobs. Collected {len(descs)} descriptions.\")\n",
                "            all_descriptions.extend(descs)\n",
                "        else:\n",
                "            print(f\"  -> No jobs found for {role}.\")\n",
                "        \n",
                "        # Be nice to APIs\n",
                "        time.sleep(2)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  -> Error scraping {role}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Total Descriptions collected: {len(all_descriptions)}\")\n",
                "\n",
                "if all_descriptions:\n",
                "    # Counter for all words\n",
                "    word_counts = Counter()\n",
                "    \n",
                "    for desc in all_descriptions:\n",
                "        tokens = clean_and_tokenize(desc)\n",
                "        word_counts.update(tokens)\n",
                "        \n",
                "    top_30 = word_counts.most_common(30)\n",
                "    \n",
                "    print(\"\\n--- Top 30 Skills / Keywords ---\")\n",
                "    for i, (word, count) in enumerate(top_30, 1):\n",
                "        print(f\"{i}. {word}: {count}\")\n",
                "else:\n",
                "    print(\"No data to analyze.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "if all_descriptions and top_30:\n",
                "    words, counts = zip(*top_30)\n",
                "    \n",
                "    plt.figure(figsize=(12, 8))\n",
                "    sns.barplot(x=list(counts), y=list(words), palette='viridis')\n",
                "    plt.title(f'Top 30 Skills for {len(roles)} Roles (n={len(all_descriptions)} jobs)')\n",
                "    plt.xlabel('Frequency')\n",
                "    plt.ylabel('Skill / Keyword')\n",
                "    plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}